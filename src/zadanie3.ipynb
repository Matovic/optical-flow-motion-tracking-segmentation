{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical flow, motion tracking, segmentation, stereo vision\n",
    "Erik Matovič  \n",
    "A solution inspired by [Open CV optical flow tutorial](https://docs.opencv.org/4.x/d4/dee/tutorial_optical_flow.html)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "#from utils import show_img, resize_img, calc_histogram_show, plt_img, equalize_hist, gamma_coorection\n",
    "\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse optical flow\n",
    "\n",
    "Visualize trajectories of moving objects.\n",
    "\n",
    "Optional task: Identify each object using a bounding box and count them.\n",
    "\n",
    "Use following functions: cv::goodFeaturesToTrack, cv::calcOpticalFlowPyrLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_optical_flow(cap: cv2.VideoCapture, out: cv2.VideoWriter, \n",
    "                        ShiTomasi_params: dict, pyrLK_params: dict) -> None:\n",
    "    # Take first frame and find corners in it\n",
    "    ret, old_frame = cap.read()\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ShiTomasi corner detection\n",
    "    corners = cv2.goodFeaturesToTrack(old_gray, **ShiTomasi_params)\n",
    "\n",
    "    # Create a mask image for drawing purposes\n",
    "    mask = np.zeros_like(old_frame)\n",
    "\n",
    "    # list of random colors\n",
    "    color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "    # Lucas-Kanade Optical Flow\n",
    "    ret, frame = cap.read()\n",
    "    while(ret):\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # calculate optical flow\n",
    "        # nextPts = 2D next points\n",
    "        # st = status vector, 1 if the the corresponding features has been found\n",
    "        nextPts, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, corners, None, **pyrLK_params)\n",
    "        \n",
    "        # Select good points based on status\n",
    "        if nextPts is not None:\n",
    "            good_new = nextPts[st==1]\n",
    "            good_old = corners[st==1]\n",
    "\n",
    "        # draw the tracks\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            if i >= len(color):\n",
    "                  i %= 10 \n",
    "\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            pt1, pt2 = (int(a), int(b)), (int(c), int(d))\n",
    "            mask = cv2.line(mask, pt1, pt2, color[i].tolist()) #, thickness=5)\n",
    "            #frame = cv2.circle(frame, pt1, 10, color[i].tolist(), -1)\n",
    "        img = cv2.add(frame, mask)\n",
    "        #cv2.imshow('frame', img)\n",
    "        #cv2.waitKey(0)\n",
    "        # write the flipped frame\n",
    "        out.write(img)\n",
    "        # Now update the previous frame and previous points\n",
    "        old_gray = frame_gray.copy()\n",
    "        corners = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "        # read next frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cap_out(video_path:str) -> tuple:\n",
    "    \"\"\"\n",
    "    returns: cv2.VideoCapture, cv2.VideoWriter \n",
    "    \"\"\"\n",
    "    # load video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    # make video writer\n",
    "    out = cv2.VideoWriter('..' + video_path[15:-4] + '.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "    return cap, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShiTomasi_params = dict(mask = None, \n",
    "    maxCorners = 10000,         # max number of corners to return\n",
    "    qualityLevel = 0.1,       # min accepted quality of img corners\n",
    "    minDistance = 1,           # min possible Euclidean distance between the returned corners\n",
    "    blockSize = 1,             # size of an average block for computing a derivative covariation matrix over each pixel neighborhood\n",
    "    useHarrisDetector = False \n",
    ")\n",
    "\n",
    "pyrLK_params = dict(winSize  = (41, 41),   # size of the search window at each pyramid lvl\n",
    "                 maxLevel = 4,          # max pyramid level number\n",
    "                 # termination criteria\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap, out = get_cap_out('../data/archive/fourway.avi')\n",
    "sparse_optical_flow(cap, out, ShiTomasi_params, pyrLK_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShiTomasi_params = dict(mask = None, \n",
    "    maxCorners = 10000,         # max number of corners to return\n",
    "    qualityLevel = 0.1,       # min accepted quality of img corners\n",
    "    minDistance = 1,           # min possible Euclidean distance between the returned corners\n",
    "    blockSize = 1,             # size of an average block for computing a derivative covariation matrix over each pixel neighborhood\n",
    "    useHarrisDetector = False \n",
    ")\n",
    "\n",
    "pyrLK_params = dict(winSize  = (41, 41),   # size of the search window at each pyramid lvl\n",
    "                 maxLevel = 4,          # max pyramid level number\n",
    "                 # termination criteria\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "cap, out = get_cap_out('../data/archive/crosswalk.avi')\n",
    "sparse_optical_flow(cap, out, ShiTomasi_params, pyrLK_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShiTomasi_params = dict(mask = None, \n",
    "    maxCorners = 10000,         # max number of corners to return\n",
    "    qualityLevel = 0.1,       # min accepted quality of img corners\n",
    "    minDistance = 1,           # min possible Euclidean distance between the returned corners\n",
    "    blockSize = 1,             # size of an average block for computing a derivative covariation matrix over each pixel neighborhood\n",
    "    useHarrisDetector = False \n",
    ")\n",
    "\n",
    "pyrLK_params = dict(winSize  = (41, 41),   # size of the search window at each pyramid lvl\n",
    "                 maxLevel = 4,          # max pyramid level number\n",
    "                 # termination criteria\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap, out = get_cap_out('../data/archive/night.avi')\n",
    "sparse_optical_flow(cap, out, ShiTomasi_params, pyrLK_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense optical flow\n",
    "\n",
    "Identify moving objects in video and draw green rectangle around them.\n",
    "\n",
    "Use downsampled video for this task if necessary for easier processing.\n",
    "\n",
    "Use following functions: cv::calcOpticalFlowFarneback\n",
    "\n",
    "[OpenCV's tutorial on how to optical flow](https://docs.opencv.org/4.x/d4/dee/tutorial_optical_flow.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion tracking Datasets\n",
    "\n",
    "Feel free to experiment with multiple videos for motion tracking. Use the following link for additional datasets - https://motchallenge.net/data/MOT15/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation using background subtraction\n",
    "\n",
    "Use background substraction methods to properly segment the moving objects from their background. Use one of the videos with static camera.\n",
    "\n",
    "Use the following approaches:\n",
    "\n",
    "    Accumulated weighted image\n",
    "\n",
    "    Mixture of Gaussian (MOG2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Cut segmentation\n",
    "\n",
    "Propose a simple method to segment a rough estimate of lateral ventricle segmentation using morphological processing and thresholding.\n",
    "\n",
    "[Link, 5 x PNG, 137 KB](https://drive.google.com/file/d/1hnQ_PHx0LhMNCMlpwCFhXVx4fFl9j_Aq/view) \n",
    "\n",
    "Use OpenCV's graph cut method to refine segmentation boundary.\n",
    "\n",
    "cv::grabCut\n",
    "\n",
    "Input has to be BGR (3 channel)\n",
    "\n",
    "Values for the mask parameter:\n",
    "\n",
    "GC_BGD = 0 - an obvious background pixels\n",
    "\n",
    "GC_FGD = 1 - an obvious foreground (object) pixel\n",
    "\n",
    "GC_PR_BGD = 2 - a possible background pixel\n",
    "\n",
    "GC_PR_FGD = 3  - a possible foreground pixel\n",
    "\n",
    "An example of GrabCut algorithm: link (note: This example uses a defined rectangle for grabcut segmentation. In our case we want to use the mask option instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
