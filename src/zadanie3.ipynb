{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical flow, motion tracking, segmentation, stereo vision\n",
    "Erik MatoviÄ  \n",
    "A solution inspired by [Open CV optical flow tutorial](https://docs.opencv.org/4.x/d4/dee/tutorial_optical_flow.html)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "#from utils import show_img, resize_img, calc_histogram_show, plt_img, equalize_hist, gamma_coorection\n",
    "\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse optical flow\n",
    "\n",
    "Visualize trajectories of moving objects.\n",
    "\n",
    "Optional task: Identify each object using a bounding box and count them.\n",
    "\n",
    "Use following functions: cv::goodFeaturesToTrack, cv::calcOpticalFlowPyrLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_optical_flow(cap: cv2.VideoCapture, out: cv2.VideoWriter, \n",
    "                        ShiTomasi_params: dict, pyrLK_params: dict) -> None:\n",
    "    # Take first frame and find corners in it\n",
    "    ret, old_frame = cap.read()\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ShiTomasi corner detection\n",
    "    corners = cv2.goodFeaturesToTrack(old_gray, **ShiTomasi_params)\n",
    "\n",
    "    # Create a mask image for drawing purposes\n",
    "    mask = np.zeros_like(old_frame)\n",
    "\n",
    "    # list of random colors\n",
    "    color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "    # Lucas-Kanade Optical Flow\n",
    "    ret, frame = cap.read()\n",
    "    while(ret):\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # calculate optical flow\n",
    "        # nextPts = 2D next points\n",
    "        # st = status vector, 1 if the the corresponding features has been found\n",
    "        nextPts, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, corners, None, **pyrLK_params)\n",
    "        \n",
    "        # Select good points based on status\n",
    "        if nextPts is not None:\n",
    "            good_new = nextPts[st==1]\n",
    "            good_old = corners[st==1]\n",
    "\n",
    "        # draw the tracks\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            if i >= len(color):\n",
    "                  i %= 10 \n",
    "\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            pt1, pt2 = (int(a), int(b)), (int(c), int(d))\n",
    "            mask = cv2.line(mask, pt1, pt2, color[i].tolist()) #, thickness=5)\n",
    "            #frame = cv2.circle(frame, pt1, 10, color[i].tolist(), -1)\n",
    "        img = cv2.add(frame, mask)\n",
    "        #cv2.imshow('frame', img)\n",
    "        #cv2.waitKey(0)\n",
    "        # write the flipped frame\n",
    "        out.write(img)\n",
    "        # Now update the previous frame and previous points\n",
    "        old_gray = frame_gray.copy()\n",
    "        corners = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "        # read next frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cap_out(video_path:str, out_root:str='..', start_idx:int=15) -> tuple:\n",
    "    \"\"\"\n",
    "    returns: cv2.VideoCapture, cv2.VideoWriter \n",
    "    \"\"\"\n",
    "    # load video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "\n",
    "    # make video writer\n",
    "    out = cv2.VideoWriter(out_root + video_path[start_idx:-4] + '.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "    return cap, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_root = '../sparse_optical_flow'\n",
    "ShiTomasi_params = dict(mask = None, \n",
    "    maxCorners = 10000,         # max number of corners to return\n",
    "    qualityLevel = 0.1,       # min accepted quality of img corners\n",
    "    minDistance = 1,           # min possible Euclidean distance between the returned corners\n",
    "    blockSize = 1,             # size of an average block for computing a derivative covariation matrix over each pixel neighborhood\n",
    "    useHarrisDetector = False \n",
    ")\n",
    "\n",
    "pyrLK_params = dict(winSize  = (41, 41),   # size of the search window at each pyramid lvl\n",
    "                 maxLevel = 4,          # max pyramid level number\n",
    "                 # termination criteria\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap, out = get_cap_out('../data/archive/fourway.avi', out_root)\n",
    "sparse_optical_flow(cap, out, ShiTomasi_params, pyrLK_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShiTomasi_params = dict(mask = None, \n",
    "    maxCorners = 10000,         # max number of corners to return\n",
    "    qualityLevel = 0.1,       # min accepted quality of img corners\n",
    "    minDistance = 1,           # min possible Euclidean distance between the returned corners\n",
    "    blockSize = 1,             # size of an average block for computing a derivative covariation matrix over each pixel neighborhood\n",
    "    useHarrisDetector = False \n",
    ")\n",
    "\n",
    "pyrLK_params = dict(winSize  = (41, 41),   # size of the search window at each pyramid lvl\n",
    "                 maxLevel = 4,          # max pyramid level number\n",
    "                 # termination criteria\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "cap, out = get_cap_out('../data/archive/crosswalk.avi', out_root)\n",
    "sparse_optical_flow(cap, out, ShiTomasi_params, pyrLK_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShiTomasi_params = dict(mask = None, \n",
    "    maxCorners = 10000,         # max number of corners to return\n",
    "    qualityLevel = 0.1,       # min accepted quality of img corners\n",
    "    minDistance = 1,           # min possible Euclidean distance between the returned corners\n",
    "    blockSize = 1,             # size of an average block for computing a derivative covariation matrix over each pixel neighborhood\n",
    "    useHarrisDetector = False \n",
    ")\n",
    "\n",
    "pyrLK_params = dict(winSize  = (41, 41),   # size of the search window at each pyramid lvl\n",
    "                 maxLevel = 4,          # max pyramid level number\n",
    "                 # termination criteria\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap, out = get_cap_out('../data/archive/night.avi', out_root)\n",
    "sparse_optical_flow(cap, out, ShiTomasi_params, pyrLK_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShiTomasi_params = dict(mask = None, \n",
    "    maxCorners = 10000,         # max number of corners to return\n",
    "    qualityLevel = 0.1,       # min accepted quality of img corners\n",
    "    minDistance = 1,           # min possible Euclidean distance between the returned corners\n",
    "    blockSize = 1,             # size of an average block for computing a derivative covariation matrix over each pixel neighborhood\n",
    "    useHarrisDetector = False \n",
    ")\n",
    "\n",
    "pyrLK_params = dict(winSize  = (41, 41),   # size of the search window at each pyramid lvl\n",
    "                 maxLevel = 4,          # max pyramid level number\n",
    "                 # termination criteria\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap, out = get_cap_out('../data/JAAD/video_0028.mp4', out_root, -15)\n",
    "ret, old_frame = cap.read()\n",
    "sparse_optical_flow(cap, out, ShiTomasi_params, pyrLK_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense optical flow\n",
    "\n",
    "Identify moving objects in video and draw green rectangle around them.\n",
    "\n",
    "Use downsampled video for this task if necessary for easier processing.\n",
    "\n",
    "Use following functions: cv::calcOpticalFlowFarneback\n",
    "\n",
    "[OpenCV's tutorial on how to optical flow](https://docs.opencv.org/4.x/d4/dee/tutorial_optical_flow.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rng\n",
    "rng.seed(12345)\n",
    "\n",
    "def dense_optical_flow(cap: cv2.VideoCapture, out: cv2.VideoWriter,\n",
    "                       farneback_params: dict) -> None:\n",
    "    ret, frame1 = cap.read()\n",
    "    prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    hsv = np.zeros_like(frame1)\n",
    "    hsv[..., 1] = 255\n",
    "    ret, frame2 = cap.read()\n",
    "    while(ret):\n",
    "        next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "        # dense optical flow by Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(\n",
    "            prev=prvs,      # first 8-bit single-channel input image \n",
    "            next=next,      # second input img with the same size and the same type as prev\n",
    "            **farneback_params)\n",
    "        #print(flow[..., 0], flow[..., 1])\n",
    "        flow[..., 0]\n",
    "        flow[..., 1]\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        mag = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)\n",
    "        #print(flow.shape)\n",
    "        # img hue according to the optical flow direction\n",
    "        hsv[..., 0] = (ang * 180) / (2 * np.pi)\n",
    "        # img value according to the normalized optical flow magnitude\n",
    "        #print('pred:\\n', mag)\n",
    "        hsv[..., 2] = cv2.normalize(mag, None, 0.0, 255.0, cv2.NORM_MINMAX)\n",
    "        #hsv = cv.rectangle(hsv, pt1, pt2, color[, thickness[, lineType[, shift]]])\n",
    "        # Convert HSV to BGR\n",
    "        #print('po:\\n', hsv[..., 2])\n",
    "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "        #print('BGR:\\n', bgr)\n",
    "        #cv.imshow('frame2', bgr)\n",
    "        gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "        canny = cv2.Canny(gray, 0, 50)\n",
    "        dilated = cv2.dilate(canny, (1, 1), iterations=0)\n",
    "        (contours, hierarchy) = cv2.findContours(\n",
    "            dilated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        #print(contours)\n",
    "\n",
    "        # loop over the contours\n",
    "        #for contour in contours:\n",
    "        #    (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            # if the contour is too small, ignore it\n",
    "        #    if w > 10 and h > 10 and w < 900 and h < 680:\n",
    "        #        cv2.rectangle(bgr, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "        #        cv2.putText(bgr, 'test', (x,y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255),1)\n",
    "\n",
    "        # Approximate contours to polygons + get bounding rects and circles\n",
    "        #contours_poly = [None]*len(contours)\n",
    "        #boundRect = [None]*len(contours)\n",
    "        #centers = [None]*len(contours)\n",
    "        #radius = [None]*len(contours)\n",
    "        for i, c in enumerate(contours):\n",
    "            # Calculate area and remove small elements\n",
    "            #area = cv2.contourArea(c)\n",
    "            #if area < 10:\n",
    "            #    continue\n",
    "            #contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "            #boundRect[i] = cv2.boundingRect(contours_poly[i])\n",
    "            (x, y, w, h) = cv2.boundingRect(c)\n",
    "            if w < 50 or h < 50 or w > 900 or h > 800:\n",
    "                    continue\n",
    "            color = (0, 255, 0) #(rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "            #cv2.drawContours(bgr, contours_poly, i, color)\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), color, 2)\n",
    "            #centers[i], radius[i] = cv.minEnclosingCircle(contours_poly[i])\n",
    "            # Draw polygonal contour + bonding rects + circles\n",
    "        #for i in range(len(contours)):\n",
    "            # Calculate area and remove small elements\n",
    "        #    (x, y, w, h) = boundRect[i]\n",
    "        #    if w < 25 or h < 25 or w > 900 or h > 800:\n",
    "        #        continue\n",
    "        #    color = (0, 255, 0) #(rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "            #cv2.drawContours(bgr, contours_poly, i, color)\n",
    "        #    cv2.rectangle(frame1, (x, y), (x + w, y + h), color, 2)\n",
    "            #bgr = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #cv2.rectangle(bgr, (x1, y1), (x2, y2), (255,0,0), 2)\n",
    "        #cv2.drawContours(bgr, contours, -1, (0, 255, 0), 2)\n",
    "        #bgr_resize = cv2.resize(frame1, (600, 400))\n",
    "        #cv2.imshow('frame2', bgr_resize)\n",
    "        #cv2.waitKey(0)\n",
    "        #break\n",
    "        out.write(frame2)\n",
    "        prvs = next\n",
    "        ret, frame2 = cap.read()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_root = '../dense_optical_flow'\n",
    "cap, out = get_cap_out('../data/archive/night.avi', out_root)\n",
    "\n",
    "farneback_params = dict(\n",
    "            flow=None,      # computed flow image, not needed, because it is returned \n",
    "            pyr_scale=0.5,  # img scale to build pyramids for each image; pyr_scale=0.5 -> each next layer is twice smaller than the previous. \n",
    "            levels=3,       # number of pyramid layers including the initial image   \n",
    "            winsize=41,     # larger values increase the robustness to image noise  \n",
    "            iterations=5,   # number of iterations the algorithm does at each pyramid level\n",
    "            poly_n=5,       # size of the pixel neighborhood; larger values mean that the image will be approximated with smoother surfaces, more robust algorithm and more blurred motion field. \n",
    "            poly_sigma=1.5, # standard deviation of the Gaussian \n",
    "            flags=0         # operation flags\n",
    ")\n",
    "dense_optical_flow(cap, out, farneback_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion tracking Datasets\n",
    "\n",
    "Feel free to experiment with multiple videos for motion tracking. Use the following link for additional datasets - https://motchallenge.net/data/MOT15/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation using background subtraction\n",
    "\n",
    "Use background substraction methods to properly segment the moving objects from their background. Use one of the videos with static camera.\n",
    "\n",
    "Use the following approaches:\n",
    "\n",
    "    Accumulated weighted image\n",
    "\n",
    "    Mixture of Gaussian (MOG2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOG2(cap: cv2.VideoCapture) -> None:\n",
    "    backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "    ret, frame = cap.read()\n",
    "    count = 0\n",
    "    while(ret):\n",
    "        mask = backSub.apply(frame)\n",
    "        \n",
    "        cv2.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n",
    "        cv2.putText(frame, str(cap.get(cv2.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "        frame_resize = cv2.resize(frame, (600, 400))\n",
    "        mask__resize = cv2.resize(mask, (600, 400))\n",
    "\n",
    "        count += 1\n",
    "        if count % 100:\n",
    "            cv2.imshow('Frame', frame_resize)\n",
    "            cv2.imshow('FG Mask', mask__resize)\n",
    "            #cv2.imshow('frame2', bgr_resize)\n",
    "            cv2.waitKey(0)\n",
    "        #    break\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_root = '../MOG2'\n",
    "cap, out = get_cap_out('../data/archive/crosswalk.avi', out_root)\n",
    "\n",
    "MOG2(cap)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Cut segmentation\n",
    "\n",
    "Propose a simple method to segment a rough estimate of lateral ventricle segmentation using morphological processing and thresholding.\n",
    "\n",
    "[Link, 5 x PNG, 137 KB](https://drive.google.com/file/d/1hnQ_PHx0LhMNCMlpwCFhXVx4fFl9j_Aq/view) \n",
    "\n",
    "Use OpenCV's graph cut method to refine segmentation boundary.\n",
    "\n",
    "cv::grabCut\n",
    "\n",
    "Input has to be BGR (3 channel)\n",
    "\n",
    "Values for the mask parameter:\n",
    "\n",
    "GC_BGD = 0 - an obvious background pixels\n",
    "\n",
    "GC_FGD = 1 - an obvious foreground (object) pixel\n",
    "\n",
    "GC_PR_BGD = 2 - a possible background pixel\n",
    "\n",
    "GC_PR_FGD = 3  - a possible foreground pixel\n",
    "\n",
    "An example of GrabCut algorithm: link (note: This example uses a defined rectangle for grabcut segmentation. In our case we want to use the mask option instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC12 dataset segmentation\n",
    "\n",
    "JPEG images: [link](https://drive.google.com/file/d/1MTgdBUwwBljzHIGz3bIqdLfu4qPx-PuP/view) \n",
    "\n",
    "Ground truth labels: [link](https://drive.google.com/file/d/1lR-Ihrg7yE0YVS9PxZTW-_XT6C8rZEnc/view)\n",
    "\n",
    "Propose a simple method for object segmentation. Pick 1-2 images from the provided dataset. You may use one or multiple segmentation methods such as:\n",
    "\n",
    "    grabcut\n",
    "\n",
    "    superpixel segmentation\n",
    "\n",
    "    floodfill\n",
    "\n",
    "    thresholding\n",
    "\n",
    "    and so on..\n",
    "\n",
    "Use provided ground truth label to compute Dice Score with your prediction (you may chose only 1 specific object for segmentation in case of multiple objects presented in the image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
